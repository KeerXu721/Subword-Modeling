{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CMU 11424/11824 Spring 2024: Reinflection and Paradigm Completion\n",
        "\n",
        "## Baselines for Unimorph Reinflection Task\n"
      ],
      "metadata": {
        "id": "oi4MsVc5arWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Datasets\n",
        "\n",
        "Data should be downloaded from Canvas and uploaded to the Colab filesystem."
      ],
      "metadata": {
        "id": "6YMD9gcja2Fq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCyOu-hkal9X",
        "outputId": "e5945383-d66d-4582-c35e-59d814156363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Archive:  dataset.zip\n",
            "   creating: dataset/\n",
            "  inflating: dataset/kbd.dev.tsv     \n",
            "  inflating: dataset/kbd.test.tsv    \n",
            "  inflating: dataset/kbd.train.tsv   \n",
            "  inflating: dataset/swc.dev.tsv     \n",
            "  inflating: dataset/swc.test.tsv    \n",
            "  inflating: dataset/swc.train.tsv   \n",
            "  inflating: dataset/xty.dev.tsv     \n",
            "  inflating: dataset/xty.test.tsv    \n",
            "  inflating: dataset/xty.train.tsv   \n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "!rm -rf dataset\n",
        "!unzip dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Measures\n",
        "\n",
        "Here, we are using exact match for evaluation."
      ],
      "metadata": {
        "id": "7Zm8gSn6b58q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "def evaluate(gold, pred):\n",
        "\n",
        "  preds = [int(unicodedata.normalize(\"NFC\",p)==unicodedata.normalize(\"NFC\",g)) for p, g in zip(pred, gold)]\n",
        "  if len(preds) == 0:\n",
        "    return 0\n",
        "  return sum(preds)/len(preds)"
      ],
      "metadata": {
        "id": "xOW7cEEfa9Gy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-Neural Baseline\n",
        "\n",
        "This method is pulled from the Unimorph non-neural baseline for Sigmorphon Reinflection Shared Task 2022."
      ],
      "metadata": {
        "id": "ReFfxXCzdisX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os, getopt, re\n",
        "from functools import wraps\n",
        "from glob import glob"
      ],
      "metadata": {
        "id": "qRTISfLidiGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hamming(s,t):\n",
        "    return sum(1 for x,y in zip(s,t) if x != y)\n",
        "\n",
        "\n",
        "def halign(s,t):\n",
        "    \"\"\"Align two strings by Hamming distance.\"\"\"\n",
        "    slen = len(s)\n",
        "    tlen = len(t)\n",
        "    minscore = len(s) + len(t) + 1\n",
        "    for upad in range(0, len(t)+1):\n",
        "        upper = '_' * upad + s + (len(t) - upad) * '_'\n",
        "        lower = len(s) * '_' + t\n",
        "        score = hamming(upper, lower)\n",
        "        if score < minscore:\n",
        "            bu = upper\n",
        "            bl = lower\n",
        "            minscore = score\n",
        "\n",
        "    for lpad in range(0, len(s)+1):\n",
        "        upper = len(t) * '_' + s\n",
        "        lower = (len(s) - lpad) * '_' + t + '_' * lpad\n",
        "        score = hamming(upper, lower)\n",
        "        if score < minscore:\n",
        "            bu = upper\n",
        "            bl = lower\n",
        "            minscore = score\n",
        "\n",
        "    zipped = zip(bu,bl)\n",
        "    newin  = ''.join(i for i,o in zipped if i != '_' or o != '_')\n",
        "    newout = ''.join(o for i,o in zipped if i != '_' or o != '_')\n",
        "    return newin, newout\n",
        "\n",
        "\n",
        "def levenshtein(s, t, inscost = 1.0, delcost = 1.0, substcost = 1.0):\n",
        "    \"\"\"Recursive implementation of Levenshtein, with alignments returned.\"\"\"\n",
        "    @memolrec\n",
        "    def lrec(spast, tpast, srem, trem, cost):\n",
        "        if len(srem) == 0:\n",
        "            return spast + len(trem) * '_', tpast + trem, '', '', cost + len(trem)\n",
        "        if len(trem) == 0:\n",
        "            return spast + srem, tpast + len(srem) * '_', '', '', cost + len(srem)\n",
        "\n",
        "        addcost = 0\n",
        "        if srem[0] != trem[0]:\n",
        "            addcost = substcost\n",
        "\n",
        "        return min((lrec(spast + srem[0], tpast + trem[0], srem[1:], trem[1:], cost + addcost),\n",
        "                   lrec(spast + '_', tpast + trem[0], srem, trem[1:], cost + inscost),\n",
        "                   lrec(spast + srem[0], tpast + '_', srem[1:], trem, cost + delcost)),\n",
        "                   key = lambda x: x[4])\n",
        "\n",
        "    answer = lrec('', '', s, t, 0)\n",
        "    return answer[0],answer[1],answer[4]\n",
        "\n",
        "\n",
        "def memolrec(func):\n",
        "    \"\"\"Memoizer for Levenshtein.\"\"\"\n",
        "    cache = {}\n",
        "    @wraps(func)\n",
        "    def wrap(sp, tp, sr, tr, cost):\n",
        "        if (sr,tr) not in cache:\n",
        "            res = func(sp, tp, sr, tr, cost)\n",
        "            cache[(sr,tr)] = (res[0][len(sp):], res[1][len(tp):], res[4] - cost)\n",
        "        return sp + cache[(sr,tr)][0], tp + cache[(sr,tr)][1], '', '', cost + cache[(sr,tr)][2]\n",
        "    return wrap\n",
        "\n",
        "\n",
        "def alignprs(lemma, form):\n",
        "    \"\"\"Break lemma/form into three parts:\n",
        "    IN:  1 | 2 | 3\n",
        "    OUT: 4 | 5 | 6\n",
        "    1/4 are assumed to be prefixes, 2/5 the stem, and 3/6 a suffix.\n",
        "    1/4 and 3/6 may be empty.\n",
        "    \"\"\"\n",
        "\n",
        "    al = levenshtein(lemma, form, substcost = 1.1) # Force preference of 0:x or x:0 by 1.1 cost\n",
        "    alemma, aform = al[0], al[1]\n",
        "    # leading spaces\n",
        "    lspace = max(len(alemma) - len(alemma.lstrip('_')), len(aform) - len(aform.lstrip('_')))\n",
        "    # trailing spaces\n",
        "    tspace = max(len(alemma[::-1]) - len(alemma[::-1].lstrip('_')), len(aform[::-1]) - len(aform[::-1].lstrip('_')))\n",
        "    return alemma[0:lspace], alemma[lspace:len(alemma)-tspace], alemma[len(alemma)-tspace:], aform[0:lspace], aform[lspace:len(alemma)-tspace], aform[len(alemma)-tspace:]\n",
        "\n",
        "\n",
        "def prefix_suffix_rules_get(lemma, form):\n",
        "    \"\"\"Extract a number of suffix-change and prefix-change rules\n",
        "    based on a given example lemma+inflected form.\"\"\"\n",
        "    lp,lr,ls,fp,fr,fs = alignprs(lemma, form) # Get six parts, three for in three for out\n",
        "\n",
        "    # Suffix rules\n",
        "    ins  = lr + ls + \">\"\n",
        "    outs = fr + fs + \">\"\n",
        "    srules = set()\n",
        "    for i in range(min(len(ins), len(outs))):\n",
        "        srules.add((ins[i:], outs[i:]))\n",
        "    srules = {(x[0].replace('_',''), x[1].replace('_','')) for x in srules}\n",
        "\n",
        "    # Prefix rules\n",
        "    prules = set()\n",
        "    if len(lp) >= 0 or len(fp) >= 0:\n",
        "        inp = \"<\" + lp\n",
        "        outp = \"<\" + fp\n",
        "        for i in range(0,len(fr)):\n",
        "            prules.add((inp + fr[:i],outp + fr[:i]))\n",
        "            prules = {(x[0].replace('_',''), x[1].replace('_','')) for x in prules}\n",
        "\n",
        "    return prules, srules\n",
        "\n",
        "\n",
        "def apply_best_rule(lemma, msd, allprules, allsrules):\n",
        "    \"\"\"Applies the longest-matching suffix-changing rule given an input\n",
        "    form and the MSD. Length ties in suffix rules are broken by frequency.\n",
        "    For prefix-changing rules, only the most frequent rule is chosen.\"\"\"\n",
        "\n",
        "    bestrulelen = 0\n",
        "    base = \"<\" + lemma + \">\"\n",
        "    if msd not in allprules and msd not in allsrules:\n",
        "        return lemma # Haven't seen this inflection, so bail out\n",
        "\n",
        "    if msd in allsrules:\n",
        "        applicablerules = [(x[0],x[1],y) for x,y in allsrules[msd].items() if x[0] in base]\n",
        "        if applicablerules:\n",
        "            bestrule = max(applicablerules, key = lambda x: (len(x[0]), x[2], len(x[1])))\n",
        "            base = base.replace(bestrule[0], bestrule[1])\n",
        "\n",
        "    if msd in allprules:\n",
        "        applicablerules = [(x[0],x[1],y) for x,y in allprules[msd].items() if x[0] in base]\n",
        "        if applicablerules:\n",
        "            bestrule = max(applicablerules, key = lambda x: (x[2]))\n",
        "            base = base.replace(bestrule[0], bestrule[1])\n",
        "\n",
        "    base = base.replace('<', '')\n",
        "    base = base.replace('>', '')\n",
        "    return base\n",
        "\n",
        "\n",
        "def numleadingsyms(s, symbol):\n",
        "    return len(s) - len(s.lstrip(symbol))\n",
        "\n",
        "\n",
        "def numtrailingsyms(s, symbol):\n",
        "    return len(s) - len(s.rstrip(symbol))"
      ],
      "metadata": {
        "id": "ZiFDtfnPd2A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang = 'xty'\n",
        "\n",
        "allprules, allsrules = {}, {}\n",
        "lines = [line.strip() for line in open(f\"dataset/{lang}.train.tsv\", \"r\") if line != '\\n']\n",
        "trainlemmas = set()\n",
        "trainmsds = set()\n",
        "\n",
        "# First, test if language is predominantly suffixing or prefixing\n",
        "# If prefixing, work with reversed strings\n",
        "prefbias, suffbias = 0,0\n",
        "for l in lines:\n",
        "  lemma, form, msd = l.split(u'\\t')\n",
        "  trainlemmas.add(lemma)\n",
        "  trainmsds.add(msd)\n",
        "  aligned = halign(lemma, form)\n",
        "  if ' ' not in aligned[0] and ' ' not in aligned[1] and '-' not in aligned[0] and '-' not in aligned[1]:\n",
        "      prefbias += numleadingsyms(aligned[0],'_') + numleadingsyms(aligned[1],'_')\n",
        "      suffbias += numtrailingsyms(aligned[0],'_') + numtrailingsyms(aligned[1],'_')\n",
        "for l in lines: # Read in lines and extract transformation rules from pairs\n",
        "    lemma, form, msd = l.split(u'\\t')\n",
        "    if prefbias > suffbias:\n",
        "        lemma = lemma[::-1]\n",
        "        form = form[::-1]\n",
        "    prules, srules = prefix_suffix_rules_get(lemma, form)\n",
        "\n",
        "    if msd not in allprules and len(prules) > 0:\n",
        "        allprules[msd] = {}\n",
        "    if msd not in allsrules and len(srules) > 0:\n",
        "        allsrules[msd] = {}\n",
        "\n",
        "    for r in prules:\n",
        "        if (r[0],r[1]) in allprules[msd]:\n",
        "            allprules[msd][(r[0],r[1])] = allprules[msd][(r[0],r[1])] + 1\n",
        "        else:\n",
        "            allprules[msd][(r[0],r[1])] = 1\n",
        "\n",
        "    for r in srules:\n",
        "        if (r[0],r[1]) in allsrules[msd]:\n",
        "            allsrules[msd][(r[0],r[1])] = allsrules[msd][(r[0],r[1])] + 1\n",
        "        else:\n",
        "            allsrules[msd][(r[0],r[1])] = 1\n",
        "\n",
        "evallines = [line.strip() for line in open(f\"dataset/{lang}.test1.tsv\", \"r\") if line != '\\n']\n",
        "outfile = open(f\"{lang}.txt\", \"w\")\n",
        "\n",
        "pred = []\n",
        "gold = []\n",
        "\n",
        "for l in evallines:\n",
        "    lemma, correct, msd = l.split(u'\\t')\n",
        "    if prefbias > suffbias:\n",
        "        lemma = lemma[::-1]\n",
        "    outform = apply_best_rule(lemma, msd, allprules, allsrules)\n",
        "    if prefbias > suffbias:\n",
        "        outform = outform[::-1]\n",
        "        lemma = lemma[::-1]\n",
        "    pred.append(outform)\n",
        "    gold.append(correct)\n",
        "\n",
        "    outfile.write(outform + \"\\n\")\n",
        "\n",
        "print(pred[:10])\n",
        "print(gold[:10])\n",
        "print(evaluate(pred, gold))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v33lgvfnd982",
        "outputId": "6a8f0bed-2ed4-4dec-8542-380b429ea28d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['kaʔ³nda⁴', 'kaʔ¹⁴nda⁴', 'ni¹-kaʔ³nda⁴', 'kaʔ¹³nda⁴', 'kaʔ⁴nda⁴', 'kaʔ³nu³', 'kaʔ¹⁴nu³', 'ni¹-kaʔ³nu³', 'kaʔ¹³nu³', 'kaʔ⁴nu³']\n",
            "['kaʔ³nda⁴', 'kaʔ¹⁴nda⁴', 'ni¹-kaʔ³nda⁴', 'kaʔ¹³nda⁴', 'kaʔ⁴nda⁴', 'kaʔ³nu³', '----', 'niʔ¹-kaʔ³nu³', 'kaʔ¹³nu³', '––']\n",
            "0.7351778656126482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Baseline\n",
        "\n",
        "This method is pulled from the Unimorph non-neural baseline for Sigmorphon Reinflection Shared Task 2022."
      ],
      "metadata": {
        "id": "SXu2vI1IE7bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/shijie-wu/neural-transducer/\n",
        "\n",
        "!pip install --upgrade torch==1.13.1"
      ],
      "metadata": {
        "id": "n_LpB2Dzf0ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "input_file = \"dataset/swc.test.tsv\"  # Replace with your actual file name\n",
        "output_file = \"dataset/swc.test1.tsv\"  # Save output as a TSV file\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as outfile:\n",
        "  reader = csv.reader(infile, delimiter=\"\\t\")  # Read input as tab-separated\n",
        "  writer = csv.writer(outfile, delimiter=\"\\t\")  # Write output as tab-separated\n",
        "\n",
        "  for row in reader:\n",
        "    if len(row) > 1:\n",
        "    # Insert \"kaʔ³nda⁴\" as the second element\n",
        "      modified_row = [row[0], \"kaʔ³nda⁴\", row[1]]\n",
        "      writer.writerow(modified_row)\n",
        "    else:\n",
        "    # Keep lines with a single element unchanged\n",
        "      writer.writerow(row)\n",
        "\n",
        "print(f\"Modification complete! Check {output_file}.\")"
      ],
      "metadata": {
        "id": "DKkcPXTtL-zc",
        "outputId": "93643eaa-c926-4765-d5d7-3feafa857f29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modification complete! Check dataset/swc.test1.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_train = \"\"\"\n",
        "#!/bin/bash\n",
        "lang=$1\n",
        "arch=${2:-tagtransformer}\n",
        "suff=$3\n",
        "\n",
        "lr=0.001\n",
        "scheduler=warmupinvsqr\n",
        "epochs=100\n",
        "warmup=100\n",
        "beta2=0.98       # 0.999\n",
        "label_smooth=0.1 # 0.0\n",
        "total_eval=50\n",
        "bs=512 # 256\n",
        "\n",
        "# transformer\n",
        "layers=4\n",
        "hs=1024\n",
        "embed_dim=256\n",
        "nb_heads=4\n",
        "#dropout=${2:-0.3}\n",
        "dropout=0.3\n",
        "ckpt_dir=checkpoints/sig22\n",
        "\n",
        "path=../dataset\n",
        "\n",
        "python src/train.py \\\n",
        "    --dataset sigmorphon17task1 \\\n",
        "    --train $path/$lang.train.tsv \\\n",
        "    --dev $path/$lang.dev.tsv \\\n",
        "    --test $path/$lang.test1.tsv \\\n",
        "    --model $ckpt_dir/$arch/$lang \\\n",
        "    --decode beam --max_decode_len 32 \\\n",
        "    --embed_dim $embed_dim --src_hs $hs --trg_hs $hs --dropout $dropout --nb_heads $nb_heads \\\n",
        "    --label_smooth $label_smooth --total_eval $total_eval \\\n",
        "    --src_layer $layers --trg_layer $layers --max_norm 1 --lr $lr --shuffle \\\n",
        "    --arch $arch --gpuid 0 --estop 1e-8 --bs $bs --epochs $epochs \\\n",
        "    --scheduler $scheduler --warmup_steps $warmup --cleanup_anyway --beta2 $beta2 --bestacc\n",
        "\"\"\"\n",
        "\n",
        "%cd neural-transducer/\n",
        "with open('run_train.sh', 'w') as f:\n",
        "  f.write(run_train)\n",
        "!make\n",
        "!bash run_train.sh xty"
      ],
      "metadata": {
        "id": "fEo4ZPAlFKxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jL4rmkOiCNxD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}