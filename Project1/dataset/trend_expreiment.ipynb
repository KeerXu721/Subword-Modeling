{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\17813\\anaconda3\\lib\\site-packages (1.12.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\17813\\anaconda3\\lib\\site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: pytorch-crf in c:\\users\\17813\\anaconda3\\lib\\site-packages (0.7.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install pytorch-crf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: Sequence of characters\n",
    "\n",
    "Output: Sequence of characters including white spaces to indicating the subword tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(filepath):\n",
    "    # given the source file path, return all input words\n",
    "    words = []\n",
    "    chars = set()\n",
    "    with open(filepath, encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            words.append(line.strip())\n",
    "            chars.update(set(line.strip()))\n",
    "    return words, chars\n",
    "\n",
    "def tag_original_word(original_word, tokenized_word):\n",
    "    # Initialize pointers for the original word and tags\n",
    "    org_idx = tok_idx = 0\n",
    "    tags = []\n",
    "    tag = \"B\"\n",
    "    # Iterate through the tokenized word\n",
    "    while tok_idx < len(tokenized_word) and org_idx < len(original_word):\n",
    "        # Skip characters not in the original word\n",
    "        if tokenized_word[tok_idx] == original_word[org_idx]:\n",
    "            tags.append(tag)\n",
    "            if tag == \"B\":\n",
    "                tag = \"I\"\n",
    "            org_idx += 1\n",
    "        elif tokenized_word[tok_idx] == \" \":\n",
    "            tag = \"B\"\n",
    "            tok_idx += 1\n",
    "        else:\n",
    "            tok_idx += 1\n",
    "\n",
    "    # Return the final tags as a string\n",
    "    return \"\".join(tags)\n",
    "\n",
    "# preparing labels for the dataset\n",
    "# Using \"B\", \"I\" labeling, B for beginning of the word, I for inside the word\n",
    "def get_labels(origional_words, tokenized_words):\n",
    "    labels = []\n",
    "    for o, t in zip(origional_words, tokenized_words):\n",
    "        labels.append(tag_original_word(o, t))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from TorchCRF import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(word, char_to_idx, is_input):\n",
    "    if is_input:\n",
    "        return [char_to_idx[ch] if ch in char_to_idx else char_to_idx['<unk>'] for ch in word]\n",
    "    return [char_to_idx['<start>']] + [char_to_idx[ch] for ch in word] + [char_to_idx['<end>']]\n",
    "\n",
    "def decode(encoded_word, idx_to_char):\n",
    "    return ''.join(idx_to_char[idx] for idx in encoded_word if idx_to_char[idx] not in ['<pad>', '<unk>', '<start>', '<end>'])\n",
    "\n",
    "def encode_whole(words, char_to_idx, is_input):\n",
    "    words_encoded = []\n",
    "    for word in words:\n",
    "        encoded_word = encode(word, char_to_idx, is_input)\n",
    "        words_encoded.append(encoded_word)\n",
    "    \n",
    "    return words_encoded\n",
    "\n",
    "def decode_whole(encoded_words, idx_to_char):   \n",
    "    decoded_words = []\n",
    "    for encoded_word in encoded_words:\n",
    "        decoded_word = decode(encoded_word, idx_to_char)\n",
    "        decoded_words.append(decoded_word)\n",
    "    \n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yoyoaibata', 'kotsatax', 'bokasai', 'ikainiki', 'kari', 'ibon', 'yoina', 'oi', 'rekena', 'orkai', 'xeki', 'janxbata', 'axea', 'bokanni', 'oinmayamai', 'jaskáribi', 'borosiko', 'maibo', 'netai', 'rayosanki', 'en', 'anibaon', 'beá', 'jakonshoko', 'yantampaketaitian', 'joá', 'jabekona', 'jainoaki', 'jotikoma', 'cepillonin', 'jaskáaki', 'quinua', 'jatíbi', 'jainxonribi', 'jeme', 'Chile', 'ichaya', 'jawekeskatinki', 'presidenteki', 'kini', 'noxa', 'potókinshaman', 'mana', 'nexaa', 'to', 'pia', 'moa', 'koinonin', 'jawékiboribi', 'jawetio', 'kirika', 'pikóribia', 'boi', 'escuelankoniax', 'ninkataxxonki', 'Martín', 'jonibaon', 'awakanki', 'xete', 'ikárin', 'sereya', 'jawen', 'yoiai', 'jawékioma', 'jawetii', 'cuadernonin', 'biá', 'raketi', 'bixon', 'merameax', 'banai', 'joniake', 'joninki', 'teekana', 'nato', 'joniani', 'manaman', 'aninkobo', 'xawi', 'xoike', 'sikita', 'yami', 'joaxxa', 'besoaxki', 'chankákaini', 'xenin', 'akaibo', 'paro', 'jawekeskataxki', 'jaskatai', 'keotaitianki', 'mapo', 'rabéra', 'piti', 'vegetariano', 'Kosi', 'Cristóbal', 'oinkasai', 'Panchanra', 'enra', 'tsinitiboribi', 'aká', 'te', 'chinia', 'bakeranonbo', 'jawekeskatiki', 'jaton', 'jaskara', 'onana', 'akonshaman', 'kaai', 'epa', 'rabin', 'joaiori', 'paráni', 'xana', 'akinairin', 'mas', 'nomi', 'bakeki', 'mato', 'jaa', 'Perúainoax', 'pakéketian', 'jankosh', 'janekana', 'escuelankoxon', 'jawe', 'petroleo', 'xoxoabo', 'jasiman', 'chosko', 'jaríbake', 'popo', 'taxnábekona', 'tsoaboki', 'jaweatiki', 'paketi', 'Américainoaxa', 'jawetianbi', 'banka', 'waiakin', 'kakasai', 'jatianki', 'ronon', 'osankinkasai', 'mesan', 'suajualinin', 'akana', 'cuídankin', 'vapor', 'jenai', 'maníshain', 'queyoa', 'basiki', 'xoxoai', 'makan', 'isini', 'bimi', 'beyamabiresa', 'xontakoki', 'bii', 'niaa', 'manishmatima', 'mananxawe', 'jakonax', 'bakishra', 'banaa', 'iamarake', 'ikaxbi', 'itikoma', 'jakátiai', 'riki', 'presidente', 'waia', 'menitiki', 'nií', 'benai', 'cuaderno', 'jakonshamanbires', 'francesnin', 'jan', 'rarás', 'jakomacha', 'Manco', 'nonki', 'ainbaon', 'paketa', 'tsisenenainbi', 'bakera', 'oshitanishoko', 'buenos', 'yonaya', 'mawata', 'jakonki', 'peonon', 'korikiya', 'keskábo', 'ochóbires', 'akintiki', 'papía', 'maira', 'mekeman', 'bake', 'pibánon', 'jistenai', 'ompaxbicho', 'bakeranonmanki', 'canadiensesbo', 'mananxawen', 'San', 'Incan', 'kexá', 'jaaiba', 'jenewe', 'chiniax', 'papan', 'aninasibo', 'manko', 'mapamea', 'Antártica', 'akinti', 'ranx', 'kikinbires', 'yokata', 'keskáribirin', 'anibobi', 'centavo', 'oinnai', 'menii', 'yokaresi', 'wametio', 'noikana', 'Perúainoaxpari', 'jawepariki', 'eara', 'chonoshoko', 'jakonwakin', 'axon', 'papá', 'reyakaa', 'osani', 'toponboax', 'asiain', 'karíbaa', 'oinxon', 'piisi', 'yora', 'chio', 'shinana', 'wetsaressibai', 'ea', 'akanai', 'keska', 'akí', 'yamékiri', 'xawen', 'shinan', 'jainoaxki', 'jaskarainoaxki', 'benabatanon', 'iketianronki', 'jawerato', 'kerosene', 'mekenshoko', 'kimisha', 'rikankin', 'pikóxon', 'jakoni', 'miara', 'bero', 'matsibires', 'xoke', 'jawekeskárin', 'pichika', 'kaí', 'niai', 'jaskatax', 'kankan', 'tsakati', 'bichi', 'sion', 'nokon', 'ransati', 'ania', 'chonka', 'ayamai', 'cuídannoxon', 'ilesos', 'sontameeti', 'nokowetsayamaa', 'isonxoman', 'atipanyamake', 'jaskáxon', 'ichama', 'jawebi', 'koshiai', 'atiya', 'jiwima', 'jawerano', 'ramara', 'texonko', 'axeamis', 'sontároyaribi', 'Perúainra', 'tsiniwe', 'inonra', 'yomerakinai', 'atoshia', 'yawishan', 'paparibi', 'quedito', 'kakinbi', 'pii', 'jawékia', 'oa', 'jaskákenbiki', 'mibé', 'yoiamaa', 'tsoabichoki', 'ikoni', 'oinmakasai', 'netetiibi', 'raroshaman', 'mawa', 'Chexe', 'empari', 'jakonma', 'jawetiishokores', 'rarebo', 'tesaa', 'miope', 'akábo', 'biai', 'wasnonbo', 'retekin', 'nawan', 'kabékona', 'oyarakinra', 'koshi', 'bitankanwen', 'axonki', 'xeati', 'rania', 'beketian', 'jaweraorin', 'chopa', 'nishin', 'jawekeskaaxonki', 'jake', 'iresa', 'jana', 'betan', 'ainbaonki', 'bewá', 'kirinko', 'yoiyai', 'ishton', 'yomerati', 'kati', 'netenki', 'jaweranki', 'ian', 'tasakan', 'ikonbiressiki', 'rama', 'xaran', 'aíbatabo', 'jawekeskarainki', 'bokana', 'vinoya', 'ike', 'yatana', 'awin', 'xoxoairibi', 'bochiki', 'piá', 'jaskarainxon', 'maestron', 'beshéakin', 'joríbakasabi', 'meshotax', 'kaketianki', 'Cápac', 'oinnax', 'shinora', 'onan', 'shinonki', 'kaíatima', 'apaokanike', 'koríki', 'siná', 'koka', 'kai', 'jiwi', 'akinkaske', 'manawe', 'mekenki', 'akinkasai', 'janea', 'tapon', 'yapabora', 'reteti', 'otitisya', 'kaaxxonki', 'jaweranorin', 'jawekeskata', 'memio', 'icha', 'mapó', 'Andes', 'teéyama', 'jonironki', 'coreano', 'autista', 'joshin', 'tetea', 'jainki', 'xetaya', 'castellanonin', 'boá', 'wai', 'manokoxota', 'xawe', 'ikátiai', 'baritianra', 'rabé', 'amichaa', 'sionbo', 'oinwe', 'Inkaronki', 'manan', 'tora', 'jatibiya', 'iráke', 'aniketian', 'oiniki', 'atires', 'beai', 'tenista', 'xoxoa', 'aninkoribi', 'Australianobo', 'atunya', 'awá', 'beyamai', 'sawea', 'caucho', 'resenenao', 'wanoa', 'kaxon', 'jaskáaxon', 'Perúainxon', 'jainxonki', 'Colombia', 'notsinhananbi', 'keyoax', 'reshoya', 'meranra', 'matsinkobiribi', 'oinke', 'jawékiaabo', 'ompax', 'apoo', 'iitaitianki', 'benakinbi', 'jonea', 'jonéxon', 'kextó', 'xakiti', 'maroti', 'yoii', 'koshiaa', 'epan', 'kexára', 'jawekeskaa', 'yoakai', 'bakebo', 'ponteti', 'América', 'meken', 'arrozya', 'jenen', 'oinyamai', 'sibá', 'anibo', 'orani', 'patax', 'Inca', 'yoiribia', 'iní', 'pekao', 'mashires', 'neeta', 'joni', 'jatíbira', 'anon', 'atiyoma', 'potakasai', 'noa', 'baritia', 'westíora', 'boshtiaki', 'xoma', 'ipaonike', 'wasa', 'jaára', 'iwanxonbi', 'jaskáa', 'merati', 'yoxanshokobo', 'panina', 'konto', 'llama', 'naman', 'kikinshaman', 'axona', 'jo', 'manaponko', 'niaxon', 'yawishanki', 'peokoota', 'boí', 'ransai', 'Españolon', 'mapéketian', 'wishawe', 'jonox', 'keenai', 'isthon', 'jawékira', 'raeananaibaon', 'beneshaman', 'bochoaxon', 'yamé', 'iwe', 'mai', 'weanshokobo', 'chibankasai', 'basi', 'iketian', 'tsekai', 'shiroatima', 'kawanketian', 'akinban', 'yapa', 'koin', 'keyábires', 'shinanyamaa', 'rani', 'xeatiya', 'jawékiatibo', 'wiso', 'jabi', 'akátiai', 'aata', 'ikoankai', 'mextea', 'boti', 'min', 'ikonwamakaskanai', 'akinai', 'jaweatin', 'tasá', 'xeaxon', 'días', 'kaa', 'join', 'maitakin', 'saweai', 'kikin', 'masa', 'iiti', 'oxebaon', 'oroi', 'Incaronki', 'ninkati', 'jeneyamaa', 'shinanti', 'nonon', 'noiti', 'ponte', 'kakátiai', 'ikaxbira', 'mapoa', 'ramiai', 'mekayao', 'jawékibo', 'shinanyares', 'beyamakanai', 'joyamai', 'Norte', 'bexnaman', 'kexeti', 'kachio', 'towei', 'meranoa', 'papaki', 'konxan', 'pikotaiori', 'vivankanwe', 'animea', 'xetan', 'oini', 'jatibiki', 'papabaon', 'ake', 'jainxon', 'jakonmatani', 'nokota', 'basimaribi', 'pikotai', 'bakeai', 'jaskáataanan', 'jaweskarainki', 'escuela', 'Diego', 'banénike', 'wetsankoribi', 'boexetinbi', 'minki', 'japonesnin', 'japones', 'yokáwe', 'bikana', 'chokai', 'pontérin', 'baná', 'jato', 'wetsa', 'Perú', 'axeke', 'shiroamita', 'xopan', 'iitai', 'oxakasi', 'tita', 'vinora', 'vicuñabiribi', 'kensho', 'abanon', 'jawéki', 'joyamarai', 'weníinakaina', 'eripanon', 'jaráke', 'ibata', 'tananon', 'jakiribi', 'bimiai', 'koiman', 'kachiokea', 'jonibo', 'potaribikana', 'joai', 'jaátian', 'yoia', 'kakin', 'raeanana', 'keyoxon', 'ikanai', 'mawáti', 'pekáo', 'anirin', 'inon', 'maestro', 'jeman', 'jane', 'oinmawe', 'yokákanaki', 'yonokana', 'jawenki', 'ebe', 'papaisin', 'ayamake', 'jaweatinka', 'roo', 'janebo', 'tsonki', 'kokoi', 'wetsabo', 'amen', 'anima', 'nokoa', 'keenyamai', 'onanti', 'tsoabo', 'shinanai', 'ninkayamai', 'shokores', 'joniki', 'oinai', 'jaskábirestani', 'jaweyoma', 'akinananai', 'rabe', 'earibi', 'rerai', 'xoboanoxon', 'ointinin', 'Tomra', 'bakeranon', 'de', 'kawe', 'mentsis', 'nexnexen', 'meraxon', 'matsires', 'onanmakatiai', 'kopiki', 'diestro', 'Chilenkoniaribi', 'poroto', 'memiokea', 'jawekeskáa', 'jawekeskáaxonki', 'minra', 'barira', 'meskó', 'mekemanki', 'winoti', 'Rosankoki', 'jema', 'ochíti', 'jonia', 'nokoke', 'jatibitianra', 'facebookya', 'tsoaki', 'aviónshokobo', 'xeni', 'chaxonbiribi', 'jatibi', 'jaábora', 'tsisenenao', 'jaweaka', 'vicuña', 'tapo', 'resenenainribi', 'jishtena', 'inoxon', 'main', 'chioshokobo', 'tawea', 'yawishen', 'barin', 'jawetirin', 'napóti', 'xeaa', 'kontonin', 'quinuaribi', 'nakai', 'ointi', 'iná', 'manaa', 'baken', 'taebicho', 'okoi', 'rashíko', 'basibires', 'akinwe', 'reteni', 'jain', 'senenketian', 'boexeketa', 'baritiatiibi', 'jeeen', 'moara', 'waxmen', 'onanyamai', 'ja', 'kexa', 'aki', 'ikasai', 'toweiki', 'xea', 'aninasinrin', 'mera', 'akai', 'akinke', 'jenjen', 'axeai', 'Brasil', 'kennai', 'manamanra', 'yantancha', 'bai', 'jakonbirestani', 'joaxki', 'ochóakana', 'atsa', 'pontéki', 'ishtoa', 'merataananki', 'biti', 'nenkéma', 'yapan', 'Francesnin', 'Europa', 'pimakatitai', 'karoya', 'neexokokasabi', 'jatíbiki', 'ikai', 'chiakaatonin', 'peyakaxon', 'paketai', 'ochomara', 'Francesninra', 'akanwe', 'jaábo', 'apo', 'ikáma', 'awé', 'jinaya', 'ikax', 'jaskáabi', 'Incaakin', 'awakan', 'meranoaxa', 'taxo', 'Inkan', 'bakish', 'jaweyomaobi', 'retea', 'joniakin', 'naí', 'yosmachaa', 'Australia', 'yomerai', 'pinon', 'toya', 'tenienteki', 'jonin', 'wasan', 'aniparo', 'tsoa', 'iti', 'papa', 'yoiti', 'japonesbo', 'aribawe', 'meraa', 'esé', 'Inka', 'onsá', 'jaskataxxonki', 'ninkata', 'baneta', 'xoati', 'bepoke', 'bexonwe', 'noike', 'pikota', 'ainbotsiki', 'chií', 'iwanke', 'pei', 'sotameeta', 'litro', 'nenké', 'janra', 'jakoma', 'manota', 'jaskáxonki', 'oinna', 'tasáyabicho', 'jawekeskáakinki', 'osana', 'matsóti', 'poiya', 'kentikan', 'jaweakiki', 'jawekeska', 'joxo', 'Perúain']\n",
      "['BIIIIIBIIB', 'BIIIIIBI', 'BIBIIBI', 'BIBBIB', 'BIII', 'BIIB', 'BIIII', 'BI', 'BIIIIB', 'BI', 'BIII', 'BIIIIB', 'B', '', 'BIIBI', 'BIIIIIIII', 'BIIIIIII', 'BIIBI', 'BIIBI', 'BIIIIBIBI', 'BB', 'BIIIBII', 'BI', 'BIIIIBIIII', 'BIIII', 'BI', 'BIBIIIIB', 'BIIIIIBI', 'BIIIBIII', 'BIIIIIIBII', 'BIIIIIBI', 'BIIIII', 'BIIIII', 'BIIIIIIBIII', 'BIII', 'BIIII', 'BIIIBI', 'BIIIIIIIIII', 'BIIIIIIIIIBI', 'BIII', 'BIII', 'BIIIBIIBIIIII', 'BIII', 'BIIII', 'BI', 'BII', 'BII', 'BIIIIBII', 'BIIIIIBIBIII', 'BIIIIII', 'BIIIII', 'BIIIBIIIB', 'BII', 'BIIIIIIBIIIIII', 'BIIIB', 'BIIIII', 'BIIIBIII', 'BIBB', 'BIII', 'BI', 'BIIIBI', 'BIIII', 'BIIBI', 'BIIIIIBII', 'BIIIIII', 'BIIIIIIIBII', 'BI', 'BIIIII', 'BIBII', 'BIIIBIII', 'BI', 'BIIIIBI', 'BIIIBBI', 'BIIBIIB', 'BIII', 'BIIIIBI', 'BIIB', 'BIIBIIBI', 'BIII', 'BIIBI', 'BIII', 'BIII', 'BIBIIB', 'B', 'BIIIIIBIIIB', 'BIIIB', 'B', 'BIII', 'BIIIIIIIIIIIBI', 'BIIIB', 'BIIIBIIIIIBI', 'BIII', 'BII', 'BIII', 'BIIIIIIIIII', 'BIII', 'BIIIIIIII', 'BIIBIIBI', 'BIIIIIBBI', 'BBBI', 'BIIIIIIBIBIII', 'B', 'BI', 'BIIIIB', 'BIIIIIIIIBI', 'BI', 'BIIII', 'BIIIIII', 'BIB', 'BIIIBIIIII', 'BIII', 'BII', 'BIIII', 'BIBIBBI', 'BII', 'BIII', 'BIIIBIBII', 'BII', 'BIII', 'BIIIBI', 'BIII', 'BII', 'BIIIBIIIII', 'BIIIBIIIII', 'BIIIIII', 'BIIIBIIB', 'BIIIIIIBIIIII', 'BIII', 'BIIIIIII', 'BI', 'BIIIBII', 'BIIIII', 'BIB', 'BIII', 'BIIIIBIIIIB', 'BIIIBIBI', 'BIIIIIIBI', 'BIIIII', 'BIIIIIIBIIII', 'BI', 'BIIII', 'BIIIBII', 'BIBIIBI', 'BIIIIIBI', 'BIIIB', 'BIIIBIIBIIBI', 'BIIIB', 'BIIIIIIIBII', 'BBIIB', 'BIIIIIBII', 'BIIII', 'BIIII', 'BII', 'BI', 'BIIIBI', 'BI', 'BIIII', 'BIIII', 'BIII', 'BIBIIIBIIIIB', 'BI', 'BII', 'BIBI', 'BIIIIIBIBIBI', 'BIIIIIIII', 'BIIIIBI', 'BIIIIIBI', 'BIIII', 'BIIIIIII', 'BIIIII', 'BIIBIII', 'BIB', 'BIII', 'BIIIIIIIII', 'BIIB', 'B', 'BII', 'BIIII', 'BIIIIIII', 'BIIIIBIIIIIBIIII', 'BIIIIIIBII', 'BIB', 'BIIII', 'BIIIBIBII', 'BIIII', 'BIBBI', 'BIII', 'BII', 'BIIIIIIIBIIBI', 'BIIIBI', 'BIIIBIIIBIIII', 'BIIIII', 'BIIIBI', 'BIIIIB', 'BIIIIBI', 'BIIBII', 'BIIIIIBI', 'BIII', 'BIIIBIIII', 'BIIIIIBI', 'BIIII', 'BIIBI', 'BIIIBII', 'BIII', 'BIB', 'BIIIIIBI', 'BIIIIBIIII', 'BIB', 'BIIIIIIIII', 'BIIIIIIIIB', 'BII', 'BIIIB', 'BIII', 'BIIBII', 'BIIIBI', 'BII', 'BIIIB', 'BIIIIIIBI', 'BIIII', 'BIIIBII', 'BIIIIIIII', 'BIIIII', 'BIII', 'BIIIIBIIII', 'BIIB', 'BIII', 'BIIIIBI', 'BIIIIII', 'BIIIBI', 'BIIII', 'BIIIBIIB', 'BIIIBII', 'BIIBIIB', 'BIIIBIIIIIBIII', 'BIIIBIIIBI', 'BIBI', 'BIIIIBIIII', 'BIIII', 'BBII', 'BIII', 'BIIIIII', 'BIIII', 'BIIB', 'BIIIBI', 'BIB', 'BIIBII', 'BIIII', 'BIII', 'BIII', 'BIIIIIB', 'BIIIIBIIIBIIB', 'BI', 'BBIIBI', 'BIIII', 'B', 'BIIIIIII', 'BIIIB', 'BIIIII', 'BIIIIIIBI', 'BIIIIIIIIIIIBI', 'BIIIIIIIIII', 'BIBIIIIBIIII', 'BIIIIIII', 'BIIIIIII', 'BII', 'BIIIIII', 'BIIIIBII', 'BIIIBII', 'BIIIII', 'BIIBI', 'BIII', 'BIIIIBIIII', 'BIII', 'BIIIIIIIIBII', 'BIIIIII', 'BI', 'BIBI', 'BIIIIIII', 'BIIIII', 'BIIIIII', 'BIIII', 'BIII', 'BIIII', 'BIIIIII', 'BIII', 'BIIIII', 'BBIIIB', 'BI', 'BIIIII', 'BIIIIBIIIB', 'BIIIBIIIIBIIII', 'BIIIIIIIB', 'BIIIIIBIIIBI', 'BIIIIIII', 'BIIIII', 'BIIIII', 'BIIIIBI', 'BIIBI', 'BIIIBI', 'BIIIIIII', 'BIIIBI', 'BIIIBII', 'BIIIIII', 'BIIIIIIBIBIII', 'BIIIBIIBI', 'BII', 'BIIBBI', 'B', 'BIBIIIB', 'BIIIIIBI', 'BIIIBIII', 'BIIIIII', 'BIBIIBI', 'BII', 'BIIIIIB', 'BI', 'BIIIIIIIBIBI', 'BIB', 'BIIBIII', 'BIIIBIIIIBI', 'BBIIB', 'BIIBIBIIBI', 'BIIIBIIII', 'BIIIBIIIII', 'BIII', 'BIIII', 'B', 'BIIIIBI', 'BIIIIIIBIIIIBII', 'BIIIII', 'B', 'BIIII', 'B', 'BIBI', 'BIIIIIBI', 'BII', 'BIIIB', 'BIB', 'B', 'BIIII', 'BIIBIBIIBI', 'BBIIBI', 'BIIII', 'BIIIB', 'BIBIIIII', 'BIIIIIIBII', 'BIIII', 'BIIIIB', 'BIIIIIIIIIIIIBI', 'BIBI', 'BBIIB', 'BIII', 'BIIII', 'BIII', 'BIII', 'BIIIIII', 'BII', 'BIIIII', 'BIIIIIII', 'BIII', 'BIIIBBI', 'BIIIII', 'BII', 'BIIB', 'BIIIBIIIIIBBI', 'BIII', 'BIIII', 'B', 'BIIIIIIIIIIIIBI', '', 'BIIIBI', 'BIB', 'BIIIIB', 'BIII', 'BI', 'BIIIIII', 'BI', 'BIIIIIIIIIII', 'BIIIIIIB', 'BIII', 'BIB', 'BIIIIIBI', 'BIBIIIIIBI', 'BIIII', 'BIIIII', 'BIIIIBI', 'BIII', 'BIIIIBBI', 'BIIIIIII', 'BBIIBIIBBI', 'BIIIII', 'BIII', 'BIII', 'BII', 'BIII', 'BBIIBIIBI', 'BIIIBI', 'BIIIIBI', 'BBIIBIIBI', 'BIIIB', 'BIIII', 'BIIIBIBI', 'BIIIII', 'BIIIIIBI', 'BIIBIBIII', 'BIIIIIIIBII', 'BI', 'BIIII', 'BIII', 'BIII', 'BIIII', 'BI', 'BIIIBIIII', 'BIIIIII', 'BIIIIII', 'BIIIII', 'BII', 'BIIIBI', 'BIIIBI', 'BIIIIIIIIIBII', 'BI', 'BII', 'BB', 'BIII', 'BI', 'BIIIIIIBBI', 'BIII', 'B', 'BIIIBI', 'BIIBI', 'BIIIBIIII', 'BIIII', 'BIII', 'BIIIIIBI', 'BIIII', 'BIIBIIIII', 'BIIIBI', 'BIIBII', 'BIBI', 'BIIIIII', 'BIIIB', 'BIIBIIBIII', '', 'BIIIBI', 'BII', 'BIBIIIB', 'BB', 'BIIIII', 'BI', 'BIIIB', 'BIBII', 'BIIIIIIII', 'BIIIBIIIII', 'BIIIIIIBI', 'BIIIIIII', 'BIIIIB', 'B', 'BIIIIBI', 'BIIIIBI', 'BIIIIIBIBIBIII', 'BIIBI', 'BIIIIIIIBI', 'BIIII', 'BIII', 'BIIBIIIIIBI', 'B', 'BIIIB', 'BIIIBII', 'BIIII', 'BIIIII', 'BIIIII', 'BIII', 'BIIIIII', 'BIIB', 'BIIIBI', 'BI', 'BIIIII', 'BIIIBI', 'BIIIIII', 'BIIIIII', 'BIIII', 'BIIIIBI', 'BIIIB', 'BIIBIIIB', 'BIII', 'BIIII', 'BIIII', 'BIIII', 'BIII', 'BIIBIIIB', 'BB', 'BIIII', 'BIIIIBII', 'BIIIB', 'BIII', 'BII', 'BBII', 'BIIBIII', 'BIIIBIIBI', 'BII', 'BIIIIII', 'BIIIIIII', 'BIIIIII', 'BIII', 'BBIIBIBI', 'BIII', 'BI', 'BBIIBIIBI', 'BIIII', 'BII', 'BIIIIBIIIIBI', 'BIIIII', 'BIIII', 'BIIII', 'BIIII', 'BII', 'BBIIB', 'BI', 'BIIIIIBII', 'BIIBII', 'BIIIIIBIBI', 'B', 'BII', 'BIIIII', 'BIIIIIIBI', 'BIIIBIIIII', 'BIIIIBI', 'BIBII', 'BIIIBI', 'BIIIII', 'BIIIIIBI', 'BIIIIIIBIBIII', 'BIIIBIIIII', 'BI', 'BIII', 'BBI', 'BII', 'BIIIBIIIIBI', 'BIIIIIBIIBI', 'BIII', 'BIBIIII', 'BIIIII', 'BII', 'BIIBI', 'BIIIBII', 'BIII', 'BIII', 'BIIIBIIII', 'BIIIB', 'BIII', 'BIIIIBI', 'BIIIIIIIIBI', 'BIII', 'BIBI', 'BB', 'BIIB', 'BIIII', 'BIIIIB', '', 'BIB', 'BIII', 'BBIIBI', 'BIIIIII', 'BIII', 'BIIBII', 'BIII', 'BII', 'BIIB', 'BIIIIBII', 'BIIIBI', 'BIIII', 'BIII', 'BIII', 'BIIBIII', 'B', 'BIIIBIIII', 'BIIIIII', 'BIIIBIIII', 'BIIIIIII', 'BIIII', 'BIIII', 'BIIII', 'BIB', 'BIIIIIBI', 'BB', 'BIIIBI', 'BIIIIII', 'BIIIIIBI', 'BIIIB', 'BIBIIIBIIBI', 'BIBIIIB', 'BIIII', 'BIIIIBII', 'BIIIII', 'BIIIII', 'BI', 'BIIIIBI', 'BIIIBI', 'BIIIII', 'BIIB', 'BIIIIBIIBI', 'BIIBII', 'BIIIB', 'BIII', 'BIIIIIBI', 'BIIIBIII', 'BBI', 'BIIIIII', 'BIIIIBIBIII', 'BII', 'BIIIBI', 'BIIIIBI', 'BIIIII', 'BIIIIIIIIIII', 'BIIIIIIIIIIBI', 'BIIIIII', 'BIIII', 'BIIIBIBI', 'BIIIIBIIBIII', 'BII', 'BIBBI', 'BIIIIIIBII', 'BIIIIII', 'BIIIBI', 'BIBIIB', 'BIIIII', 'BIIIIBII', 'BIII', 'BIII', 'BIIII', 'BIII', 'BIIII', 'BII', 'BIIII', 'BIIBI', 'BIIBIIB', 'BIII', 'BIIIBI', 'BIIIIIBIBIII', 'BIIIII', 'BBIIII', 'BIIIII', 'BIBIII', 'BIIIIBB', 'BIIBIIII', 'BIB', 'BBIII', 'BIIIBII', 'BIIIIIII', 'BIIIBI', 'BIIBII', 'BIIIIIBII', 'BIIIBI', 'B', 'BIBI', 'BI', 'BIIB', 'BIBII', 'BIIBIIIB', 'B', 'BIBIBI', 'BIIIII', 'BIIII', 'BIIBII', 'BIIB', 'BIIIIII', 'BIIIB', 'BIII', 'BIIBIBI', 'BIIIBIIBBI', 'BIIIBIIB', 'BIIIBBI', 'BBI', 'BII', 'BBIIIBI', 'BIIIIIIB', 'BII', 'BIIIBI', 'BIIBBI', 'BIIII', 'BIIIIBI', 'BIII', 'BIIBI', 'BI', 'BIIIBIIIB', 'BIIIII', 'BIIIBI', 'BIIIIIBI', 'BIIIIBIIIB', 'BIIIIIII', 'BIIIBI', 'BIIBI', 'BIIIIBIIIIBIII', 'BIIIBIII', 'BIIIBIIIBI', 'BII', 'BIBIII', 'B', 'BIIIIBIIII', 'BIIIIBII', 'BIIBI', 'BIIIIIIII', 'BI', 'BIBI', 'BIIIIII', 'BIIIIIBI', 'B', 'B', 'BIIIBIBIIIBI', 'BIIIBI', 'BIIIIII', 'BIIIIBIIIIIBIII', 'BIIIII', 'BIIIIBII', 'BIIIIIIIII', 'BIIIIIII', 'BIBBI', 'BIIIBI', 'BIIII', 'BIIIBIIBI', 'BIIIII', 'BIIIBIIBI', 'BIII', 'BIIIII', 'BIIIB', 'BII', 'BIIIIIIIIIBI', 'BIIIIIIIBI', 'BIIBBI', 'BIIIIBIIIIBI', 'BIII', 'BIIIIBBIBIII', 'BIIIII', 'BI', 'BIIIIIII', 'BIIII', 'BIIIII', 'BIII', 'BIIIIIIIBIBIII', 'BIIIIIIB', 'BBIIII', 'BIIB', 'BI', 'BIBII', 'BIIIIIBI', 'BIIIB', 'BIIIIIBII', 'BIIIII', 'B', 'BIIIIBII', 'BIIIIIBIII', 'BI', 'BIIII', 'BII', 'BI', 'BIIIB', 'BIIBIIII', 'BIII', 'BIIIIII', 'BIIIBIIII', 'BIIIBI', 'BIIIBI', 'BIII', 'BIIIIBIIIII', 'BII', 'BIIIIIIBIIII', 'BIIII', 'BIIBI', 'BIIIII', 'BIIIBIIIB', 'BI', 'BII', 'B', 'BIBIBI', 'BIIIIII', 'BII', 'BIIIIIIBBII', 'BIII', 'B', 'BII', 'BIIIII', 'B', 'BIIIII', 'BIIIBI', 'BIIIBIIBI', 'BIIIIIBII', 'BII', 'BIIIIBIIIIBIII', 'BIIBBI', 'BIIIIBIIB', 'BIII', 'BIIIIBI', 'B', 'BIIIIBIIIIBI', 'BIII', 'BIIIIII', 'BIIIB', '', 'BIIIII', 'BIBIBIIIIBI', 'BIIIBI', 'BII', 'BII', 'BIBI', 'BIIIIIIIBIII', 'B', 'BII', 'BII', 'BIIII', 'BBIIBI', 'BI', 'BII', 'BI', 'BB', 'BIIIBI', 'BIBI', 'BIIII', '', 'BIIBII', 'BIIIIBIIB', 'BIII', '', 'BIIIII', 'BIIIIIIIBBI', 'BIIIB', 'BIIIIBII', 'BII', 'BIIIIBIII', 'BIIIIIIII', 'BIIIIII', 'BIBII', 'BIII', 'BIIIIIIIBI', 'BIIIB', 'BIIIB', 'BIIIIII', 'BIII', 'BII', 'BIII', 'BIIII', 'BIIIIIIBI', 'BBIIIBI', 'B', 'BII', 'BIII', 'BIII', 'BIIIIIIIIBIII', 'BI', 'BIIIIB', 'BIIII', 'BIIIBI', 'BIBIIBI', 'BIIBI', 'BIIIIB', 'BIIIIBIIBI', 'BIII', 'BBIIBI', 'BII', 'BIIIBIIIB', 'BIIII', 'BIIII', 'BIBBI', 'BIIIBI', 'BIIIIB', 'BIIIIIIIBI', 'BIIIB', 'BIIIBIBIIII', 'BIIIIIIIIIIIIBI', 'BIB', 'BIIIIII', 'BIIBI', 'B', 'BIIIIIIBI', 'BIIIIIIII', 'BIII', 'BIIIBII']\n",
      "Counter({'I': 3815, 'B': 1479})\n",
      "['jainoax', 'yakata', 'tekíbo', 'raankana', 'soanon', 'oinberibai', 'miaki', 'potani', 'Ecuador', 'jaskataxki', 'jawékiakin', 'alemanra', 'boo', 'kenai', 'bukeya', 'mecha', 'jakonbires', 'tama', 'ensalada', 'mekayaokea', 'jaweranoki', 'matsi', 'iwana', 'beneki', 'payari', 'mayatai', 'isinkonama', 'keskáakin', 'chama', 'teetai', 'parana', 'kakinki', 'benawe', 'ani', 'ni', 'iká', 'bakeribi', 'Santa', 'titan', 'Mananxawe', 'Huayna', 'iketianki', 'jai', 'España', 'xoxoi', 'paranribia', 'jaskáaxonki', 'Colón', 'apendicitisya', 'joto', 'ati', 'Yoáshiko', 'jaweki', 'yoyo', 'koshibirestani', 'chibanban', 'rinko', 'japaonike', 'kenawe', 'moatianronki', 'iikinki', 'kokana', 'tee', 'jakon', 'karíbaparibanon', 'ichaira', 'axébiribiki', 'ikáki', 'jikiamapainon', 'bikanai', 'akasai', 'maton', 'rikan', 'ewa', 'chibanai', 'inonbires', 'noia', 'toota', 'bueno', 'keská', 'maxó', 'chapatabo', 'yoxanshoko', 'kaxonki', 'ibonra', 'shino', 'iso', 'kanana', 'aa', 'Tomki', 'jatíribi', 'galletabo', 'kairin', 'jara', 'isakan', 'moára', 'xeteai', 'jaáribi', 'pian', 'non', 'mexka', 'jaweanoaki', 'maxko', 'yoiarin', 'tooribia', 'pekáoki', 'frances', 'tanti', 'nete', 'jaweraki']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# labels = get_labels('dataset/shp.train.tgt')\n",
    "input_words, input_chars = get_words('dataset/shp.train.src')\n",
    "output_words, output_chars = get_words('dataset/shp.train.tgt')\n",
    "print(input_words)\n",
    "train_labels = get_labels(input_words, output_words)\n",
    "print(train_labels)\n",
    "print(Counter(''.join(train_labels)))\n",
    "\n",
    "val_input_words, val_input_chars = get_words('dataset/shp.dev.src')\n",
    "val_output_words, val_output_chars = get_words('dataset/shp.dev.tgt')\n",
    "\n",
    "val_labels = get_labels(val_input_words, val_output_words)\n",
    "\n",
    "test_words, test_chars = get_words('dataset/shp.test.src')\n",
    "print(test_words)\n",
    "\n",
    "# get char to index mapping and index to char mapping\n",
    "chars = set(input_chars)\n",
    "chars.update(set(output_chars))\n",
    "chars.update(set(['<pad>', '<unk>', ' ']))  # include padding, unknown, and space\n",
    "\n",
    "vocab_size = len(input_chars)\n",
    "\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}\n",
    "\n",
    "label_to_idx = {'B': 0, 'I': 1, '<pad>': 2, '<start>': 3, '<end>': 4}\n",
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "\n",
    "encoded_origin = encode_whole(input_words, char_to_idx, is_input=True)\n",
    "encoded_tokenized = encode_whole(train_labels, label_to_idx, is_input=False)\n",
    "encoded_val = encode_whole(val_input_words, char_to_idx, is_input=True)\n",
    "encoded_val_tokenized = encode_whole(val_labels, label_to_idx, is_input=False)\n",
    "\n",
    "# shape: seq_num, max_len\n",
    "padded_origin = pad_sequence([torch.tensor(word) for word in encoded_origin], batch_first=True, padding_value=char_to_idx['<pad>'])\n",
    "padded_tokenized = pad_sequence([torch.tensor(tok) for tok in encoded_tokenized], batch_first=True, padding_value=label_to_idx['<pad>'])\n",
    "\n",
    "padded_val = pad_sequence([torch.tensor(word) for word in encoded_val], batch_first=True, padding_value=char_to_idx['<pad>'])\n",
    "padded_val_tokenized = pad_sequence([torch.tensor(tok) for tok in encoded_val_tokenized], batch_first=True, padding_value=label_to_idx['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, words, tokenized):\n",
    "        self.words = words\n",
    "        self.tokenized = tokenized\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx]\n",
    "        tokenized = self.tokenized[idx]\n",
    "        # print(word, label)\n",
    "        return word, tokenized\n",
    "\n",
    "dataset = CharDataset(padded_origin, padded_tokenized)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "val_dataset = CharDataset(padded_val, padded_val_tokenized)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=20, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizationSeq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, output_size, num_layers=1):\n",
    "        super(TokenizationSeq2Seq, self).__init__()\n",
    "        \n",
    "        # Embedding layer to convert characters into embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Encoder (e.g., LSTM or GRU)\n",
    "        self.encoder = nn.LSTM(embed_dim, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Decoder (e.g., LSTM or GRU)\n",
    "        self.decoder = nn.LSTM(embed_dim, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to predict token labels (B, I, <pad>, <start>, <end>)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input_seq, target_seq=None, teacher_forcing_ratio=0.5):\n",
    "        # Step 1: Embed the input sequence (character indices)\n",
    "        embedded = self.embedding(input_seq)  # Shape: [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # Step 2: Pass the embedded sequence through the encoder (LSTM)\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(embedded)  # Shape: [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # Step 3: Initialize the decoder output\n",
    "        batch_size = input_seq.size(0)\n",
    "        target_len = target_seq.size(1) if target_seq is not None else input_seq.size(1)\n",
    "        outputs = torch.zeros(batch_size, target_len, self.fc.out_features).to(input_seq.device)\n",
    "        \n",
    "        # First input to the decoder is the <start> token (or use teacher forcing)\n",
    "        decoder_input = target_seq[:, 0] if target_seq is not None else torch.ones(batch_size).long().to(input_seq.device)\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Step 4: Embed the decoder input (which is either target token or predicted token)\n",
    "            decoder_embedded = self.embedding(decoder_input).unsqueeze(1)\n",
    "            \n",
    "            # Step 5: Pass the embedded decoder input through the decoder (LSTM)\n",
    "            decoder_output, (hidden, cell) = self.decoder(decoder_embedded, (hidden, cell))\n",
    "            \n",
    "            # Step 6: Apply the fully connected layer to get label predictions\n",
    "            output = self.fc(decoder_output.squeeze(1))  # Shape: [batch_size, output_size]\n",
    "            outputs[:, t, :] = output\n",
    "            \n",
    "            # Step 7: Decide whether to use teacher forcing\n",
    "            # if target_seq is not None and torch.rand(1).item() < teacher_forcing_ratio:\n",
    "            #     decoder_input = target_seq[:, t]  # Teacher forcing\n",
    "            # else:\n",
    "            decoder_input = output.argmax(1)  # Predicted token (most likely label)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, num_epochs):\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq, target_seq)\n",
    "            # Reshape to [batch_size * seq_len, vocab_size]\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            target_seq = target_seq.view(-1)\n",
    "            \n",
    "            loss = criterion(output, target_seq)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for input_seq, target_seq in val_dataloader:\n",
    "                input_seq = input_seq.to(torch.device('cpu'))\n",
    "                target_seq = target_seq.to(torch.device('cpu'))\n",
    "\n",
    "                output = model(input_seq, target_seq)\n",
    "                output = output.view(-1, output.size(-1))\n",
    "                target_seq = target_seq.view(-1)\n",
    "\n",
    "                loss = criterion(output, target_seq)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_seq2seq_model.pth\")\n",
    "            print(f\"Best model saved with validation loss: {best_loss:.4f}\")\n",
    "\n",
    "# Assign weights inversely proportional to the class frequency\n",
    "weight_I = 1.0 / 3815   # for shp.train.tgt\n",
    "weight_B = 1.0 / 1479\n",
    "\n",
    "# weight_I = 1.0 / 3280  # for tar.train.tgt\n",
    "# weight_B = 1.0 / 1504\n",
    "\n",
    "# Normalize weights to ensure that they sum to 1 (optional, but helpful)\n",
    "total_weight = weight_I + weight_B\n",
    "weight_I /= total_weight\n",
    "weight_B /= total_weight\n",
    "class_weights = torch.tensor([weight_B, weight_I, 0.0, 0.0, 0.0])  # B -> 0, I -> 1\n",
    "\n",
    "vocab_size = len(char_to_idx)\n",
    "output_size = len(label_to_idx)\n",
    "model = TokenizationSeq2Seq(vocab_size, 50, 100, output_size)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=char_to_idx['<pad>'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 15\n",
    "\n",
    "train(model, dataloader, optimizer, criterion, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jainoax', 'yakata', 'tekíbo', 'raankana', 'soanon', 'oinberibai', 'miaki', 'potani', 'Ecuador', 'jaskataxki', 'jawékiakin', 'alemanra', 'boo', 'kenai', 'bukeya', 'mecha', 'jakonbires', 'tama', 'ensalada', 'mekayaokea', 'jaweranoki', 'matsi', 'iwana', 'beneki', 'payari', 'mayatai', 'isinkonama', 'keskáakin', 'chama', 'teetai', 'parana', 'kakinki', 'benawe', 'ani', 'ni', 'iká', 'bakeribi', 'Santa', 'titan', 'Mananxawe', 'Huayna', 'iketianki', 'jai', 'España', 'xoxoi', 'paranribia', 'jaskáaxonki', 'Colón', 'apendicitisya', 'joto', 'ati', 'Yoáshiko', 'jaweki', 'yoyo', 'koshibirestani', 'chibanban', 'rinko', 'japaonike', 'kenawe', 'moatianronki', 'iikinki', 'kokana', 'tee', 'jakon', 'karíbaparibanon', 'ichaira', 'axébiribiki', 'ikáki', 'jikiamapainon', 'bikanai', 'akasai', 'maton', 'rikan', 'ewa', 'chibanai', 'inonbires', 'noia', 'toota', 'bueno', 'keská', 'maxó', 'chapatabo', 'yoxanshoko', 'kaxonki', 'ibonra', 'shino', 'iso', 'kanana', 'aa', 'Tomki', 'jatíribi', 'galletabo', 'kairin', 'jara', 'isakan', 'moára', 'xeteai', 'jaáribi', 'pian', 'non', 'mexka', 'jaweanoaki', 'maxko', 'yoiarin', 'tooribia', 'pekáoki', 'frances', 'tanti', 'nete', 'jaweraki']\n",
      "['BBIIIII', 'BBIIII', 'BBIIII', 'BBIIIIII', 'BBIIII', 'BBIIIBIIII', 'BBIII', 'BBIIII', 'BBIIIII', 'BBIIIIIIII', 'BBIIIIIIII', 'BBIIIIII', 'BBI', 'BBIII', 'BBIIII', 'BBIII', 'BBIIIIIIII', 'BBII', 'BBIIIIII', 'BBIIIIIIII', 'BBIIIIIIII', 'BBIII', 'BBIII', 'BBIIII', 'BBIIII', 'BBIIIII', 'BBIIIIIIII', 'BBIIIIIII', 'BBIII', 'BBIIII', 'BBIIII', 'BBIIIII', 'BBIIII', 'BBI', 'BB', 'BBI', 'BBIIIIII', 'BBIII', 'BBIII', 'BBIIIIIII', 'BBIIII', 'BBIIIIIII', 'BBI', 'BBIIII', 'BBIII', 'BBIIIIIIII', 'BBIIIIIIIII', 'BBIII', 'BBIIIIIIIIIII', 'BBII', 'BBI', 'BBIIIIII', 'BBIIII', 'BBII', 'BBIIIIIIIIIIII', 'BBIIIIIII', 'BBIII', 'BBIIIIIII', 'BBIIII', 'BBIIIIIIIIII', 'BBIIIII', 'BBIIII', 'BBI', 'BBIII', 'BBIIIIIIIIIIIII', 'BBIIIII', 'BBIIIIIIIII', 'BBIII', 'BBIIIIIIIIIII', 'BBIIIBI', 'BBIIIB', 'BBIII', 'BBIII', 'BBI', 'BBIIIIII', 'BBIIIIIII', 'BBII', 'BBIII', 'BBIII', 'BBIII', 'BBII', 'BBIIIIIII', 'BBIIIIIIII', 'BBIIIII', 'BBIIII', 'BBIII', 'BBI', 'BBIIII', 'BB', 'BBIII', 'BBIIIIII', 'BBIIIIIII', 'BBIIII', 'BBII', 'BBIIII', 'BBIII', 'BBIIII', 'BBIIIII', 'BBII', 'BBI', 'BBIII', 'BBIIIIIIII', 'BBIII', 'BBIIIII', 'BBIIIIII', 'BBIIIII', 'BBIIIII', 'BBIII', 'BBII', 'BBIIIIII']\n",
      "[' j ainoax', ' y akata', ' t ekíbo', ' r aankana', ' s oanon', ' o inbe ribai', ' m iaki', ' p otani', ' E cuador', ' j askataxki', ' j awékiakin', ' a lemanra', ' b oo', ' k enai', ' b ukeya', ' m echa', ' j akonbires', ' t ama', ' e nsalada', ' m ekayaokea', ' j aweranoki', ' m atsi', ' i wana', ' b eneki', ' p ayari', ' m ayatai', ' i sinkonama', ' k eskáakin', ' c hama', ' t eetai', ' p arana', ' k akinki', ' b enawe', ' a ni', ' n i', ' i ká', ' b akeribi', ' S anta', ' t itan', ' M ananxawe', ' H uayna', ' i ketianki', ' j ai', ' E spaña', ' x oxoi', ' p aranribia', ' j askáaxonki', ' C olón', ' a pendicitisya', ' j oto', ' a ti', ' Y oáshiko', ' j aweki', ' y oyo', ' k oshibirestani', ' c hibanban', ' r inko', ' j apaonike', ' k enawe', ' m oatianronki', ' i ikinki', ' k okana', ' t ee', ' j akon', ' k aríbaparibanon', ' i chaira', ' a xébiribiki', ' i káki', ' j ikiamapainon', ' b ikan ai', ' a kasa i', ' m aton', ' r ikan', ' e wa', ' c hibanai', ' i nonbires', ' n oia', ' t oota', ' b ueno', ' k eská', ' m axó', ' c hapatabo', ' y oxanshoko', ' k axonki', ' i bonra', ' s hino', ' i so', ' k anana', ' a a', ' T omki', ' j atíribi', ' g alletabo', ' k airin', ' j ara', ' i sakan', ' m oára', ' x eteai', ' j aáribi', ' p ian', ' n on', ' m exka', ' j aweanoaki', ' m axko', ' y oiarin', ' t ooribia', ' p ekáoki', ' f rances', ' t anti', ' n ete', ' j aweraki']\n"
     ]
    }
   ],
   "source": [
    "def predict(model, input_word, char_to_idx, label_to_idx, max_len, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Encode the input word into a sequence of indices\n",
    "    input_seq = [char_to_idx[char] for char in input_word if char in char_to_idx] # Skip unknown characters\n",
    "    input_seq = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    # Initialize the target sequence with <start> token\n",
    "    target_seq = torch.tensor([label_to_idx['<start>']], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            # Pass input and target sequence to the model\n",
    "            output = model(input_seq, target_seq)\n",
    "\n",
    "            # Get the index of the most probable token at the last time step\n",
    "            next_token = output[:, -1, :].argmax(dim=-1)\n",
    "\n",
    "            # Append the predicted token to the target sequence\n",
    "            target_seq = torch.cat([target_seq, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "            # Stop if <end> token is predicted\n",
    "            if next_token.item() == label_to_idx['<end>']:\n",
    "                break\n",
    "\n",
    "    # Decode the target sequence indices back into characters\n",
    "    decoded_tokens = ''.join([idx_to_label[idx.item()] for idx in target_seq[0][1:]])  # Skip <start> token\n",
    "    return decoded_tokens.replace('<end>', '').strip()\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_seq2seq_model.pth\"))\n",
    "\n",
    "# Predict tokenization for a test word\n",
    "prediction = []\n",
    "for word in test_words:\n",
    "    predicted = predict(model, word, char_to_idx, label_to_idx, max_len=len(word))\n",
    "    prediction.append(predicted)\n",
    "\n",
    "def decode_tag(words, tags):\n",
    "    decoded = []\n",
    "    for word, tag in zip(words, tags):\n",
    "        decoded_word = []\n",
    "        for w, t in zip(word, tag):\n",
    "            if t == 'B':\n",
    "                decoded_word.append(' ')\n",
    "            decoded_word.append(w)\n",
    "        decoded.append(''.join(decoded_word))\n",
    "    return decoded\n",
    "\n",
    "decoded = decode_tag(test_words, prediction)\n",
    "print(test_words)\n",
    "print(prediction)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the decoded words to a file with name pred_shp.test.tgt\n",
    "with open('pred_shp.test.tgt', 'w') as file:\n",
    "    for word in decoded:\n",
    "        file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0044742729306487695\n"
     ]
    }
   ],
   "source": [
    "def run_eval(golds, preds):\n",
    "   tp = 0\n",
    "   fp = 0\n",
    "   fn = 0\n",
    "   for g, p in zip(golds, preds):\n",
    "      g_bag = g.strip().split(\" \")\n",
    "      p_bag = p.strip().split(\" \")\n",
    "      tp += sum([1 for i in p_bag if i in g_bag])\n",
    "      fp += sum([1 for i in p_bag if not i in g_bag])\n",
    "      fn += sum([1 for i in g_bag if not i in p_bag])\n",
    "   precision = tp / (tp + fp)\n",
    "   recall = tp / (tp + fn)\n",
    "   if precision == 0 or recall == 0:\n",
    "      f1 = 0\n",
    "   else:\n",
    "      f1 = 2 / ((1/precision) + (1/recall))\n",
    "   return f1\n",
    "\n",
    "f1 = run_eval(val_output_words, decoded)\n",
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
